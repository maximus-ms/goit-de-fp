{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Building an End-to-End Streaming Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, round, to_json, from_json, struct, avg, lit\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    ")\n",
    "\n",
    "from configs import kafka_config, jdbc_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_EVENT_RESULTS_IX = 0\n",
    "TOPIC_GROUP_RESULTS_IX = 1\n",
    "\n",
    "topic_names = [\n",
    "    f'{kafka_config['name']}_de_fp_athlete_event_results',\n",
    "    f'{kafka_config['name']}_de_fp_group_results',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open to delete all topics with name \"kafka_config['name']*\"\n",
    "if False:\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=kafka_config['bootstrap_servers'][0],\n",
    "        security_protocol=kafka_config['security_protocol'],\n",
    "        sasl_mechanism=kafka_config['sasl_mechanism'],\n",
    "        sasl_plain_username=kafka_config['username'],\n",
    "        sasl_plain_password=kafka_config['password']\n",
    "    )\n",
    "\n",
    "    for topic in admin_client.list_topics():\n",
    "        if kafka_config['name'] in topic:\n",
    "            print(f'Deleting topic: {topic}')\n",
    "            admin_client.delete_topics(topics=[topic])\n",
    "\n",
    "    admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics are created successfully.\n",
      "maksymp_de_fp_athlete_event_results\n",
      "maksymp_de_fp_group_results\n"
     ]
    }
   ],
   "source": [
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=kafka_config['bootstrap_servers'][0],\n",
    "    security_protocol=kafka_config['security_protocol'],\n",
    "    sasl_mechanism=kafka_config['sasl_mechanism'],\n",
    "    sasl_plain_username=kafka_config['username'],\n",
    "    sasl_plain_password=kafka_config['password']\n",
    ")\n",
    "\n",
    "num_partitions = 2\n",
    "replication_factor = 1\n",
    "new_topics = [ NewTopic(name=n, num_partitions=num_partitions, replication_factor=replication_factor) for n in topic_names ]\n",
    "\n",
    "try:\n",
    "    admin_client.create_topics(new_topics=new_topics, validate_only=False)\n",
    "    print(f\"Topics are created successfully.\")\n",
    "    [print(topic) for topic in admin_client.list_topics() if kafka_config['name'] in topic]\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packet to work with Kafka in Spark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/01/31 00:25:00 WARN Utils: Your hostname, AMD resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/01/31 00:25:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/max/.ivy2/cache\n",
      "The jars for the packages stored in: /home/max/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e17c35b9-a7df-4c41-a9c1-a47be17762c5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 302ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e17c35b9-a7df-4c41-a9c1-a47be17762c5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/8ms)\n",
      "25/01/31 00:25:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars\", \"mysql-connector-j-8.0.32.jar\") \\\n",
    "    .appName(\"DE_FP\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 00:25:13 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jdbc_df(jdbc_table, limit=None):\n",
    "    df = spark.read.format('jdbc').options(\n",
    "                url = f'{jdbc_config['url']}/olympic_dataset',\n",
    "                driver = 'com.mysql.cj.jdbc.Driver',\n",
    "                dbtable = jdbc_table,\n",
    "                user = jdbc_config['user'],\n",
    "                password = jdbc_config['password']) \\\n",
    "            .load()\n",
    "    if limit:\n",
    "        df = df.limit(limit)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load `athlete_bio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "athlete_bio_raw_df = get_jdbc_df(\"athlete_bio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "|athlete_id|             name|   sex|          born|height|weight|          country|country_noc|         description|       special_notes|\n",
      "+----------+-----------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "|     65649|     IvankaBonova|Female|    4April1949| 166.0|    55|         Bulgaria|        BUL|PersonalBest40053...|                 nan|\n",
      "|    112510| NataliyaUryadova|Female|   15March1977| 184.0|    70|RussianFederation|        RUS|                 nan|ListedinOlympians...|\n",
      "|    114973| EssaIsmailRashed|  Male|14December1986| 165.0|    55|            Qatar|        QAT|PersonalBest10000...|ListedinOlympians...|\n",
      "|     30359|        PterBoros|  Male| 12January1908|      |   nan|          Hungary|        HUN|Between1927and193...|                 nan|\n",
      "|     50557|     RudolfPiowat|  Male|   28April1900|      |   nan|   Czechoslovakia|        TCH|RudolfPiowatyjoin...|                 nan|\n",
      "|    146111|SvetlanaKholomina|Female| 9November1997|      |   nan|              ROC|        ROC|                 nan|                 nan|\n",
      "|    133041|  VincentRiendeau|  Male|13December1996| 178.0|    68|           Canada|        CAN|                 nan|ListedinOlympians...|\n",
      "|    110425|       TanjaMorel|Female|  4October1975| 164.0|    58|      Switzerland|        SUI|                 nan|ListedinOlympians...|\n",
      "|    110705|   MaksimShabalin|  Male| 25January1982| 183.0|    76|RussianFederation|        RUS|                 nan|ListedinOlympians...|\n",
      "|     54541|          GRegter|  Male|    6March1916|      |   nan|      Netherlands|        NED|                 nan|                 nan|\n",
      "+----------+-----------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athlete_bio_raw_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Filter `athlete_bio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "|athlete_id|               name|   sex|          born|height|weight|          country|country_noc|         description|       special_notes|\n",
      "+----------+-------------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "|     65649|       IvankaBonova|Female|    4April1949| 166.0|  55.0|         Bulgaria|        BUL|PersonalBest40053...|                 nan|\n",
      "|    112510|   NataliyaUryadova|Female|   15March1977| 184.0|  70.0|RussianFederation|        RUS|                 nan|ListedinOlympians...|\n",
      "|    114973|   EssaIsmailRashed|  Male|14December1986| 165.0|  55.0|            Qatar|        QAT|PersonalBest10000...|ListedinOlympians...|\n",
      "|    133041|    VincentRiendeau|  Male|13December1996| 178.0|  68.0|           Canada|        CAN|                 nan|ListedinOlympians...|\n",
      "|    110425|         TanjaMorel|Female|  4October1975| 164.0|  58.0|      Switzerland|        SUI|                 nan|ListedinOlympians...|\n",
      "|    110705|     MaksimShabalin|  Male| 25January1982| 183.0|  76.0|RussianFederation|        RUS|                 nan|ListedinOlympians...|\n",
      "|     56266|       GoYeongChang|  Male|   21March1926| 167.0|  75.0|  RepublicofKorea|        KOR|                 nan|                 nan|\n",
      "|     93334|     CraigHutchison|  Male|     26May1975| 198.0|  97.0|           Canada|        CAN|                 nan|ListedinOlympians...|\n",
      "|    146013|       RaquelQueirs|Female|    4March2000| 167.0|  56.0|         Portugal|        POR|                 nan|                 nan|\n",
      "|    109912|VyacheslavKurginyan|  Male|22December1986| 170.0|  65.0|RussianFederation|        RUS|                 nan|ListedinOlympians...|\n",
      "+----------+-------------------+------+--------------+------+------+-----------------+-----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athlete_bio_df = athlete_bio_raw_df \\\n",
    "    .withColumn(\"height\", col(\"height\").cast(FloatType())) \\\n",
    "    .withColumn(\"weight\", col(\"weight\").cast(FloatType())) \\\n",
    "    .fillna({\"height\": 0, \"weight\": 0}) \\\n",
    "    .filter((col(\"height\") != 0) & (col(\"weight\") != 0))\n",
    "\n",
    "athlete_bio_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+\n",
      "|athlete_id|   sex|height|weight|\n",
      "+----------+------+------+------+\n",
      "|     65649|Female| 166.0|  55.0|\n",
      "|    112510|Female| 184.0|  70.0|\n",
      "|    114973|  Male| 165.0|  55.0|\n",
      "|    133041|  Male| 178.0|  68.0|\n",
      "|    110425|Female| 164.0|  58.0|\n",
      "+----------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athlete_bio_short_df = athlete_bio_df.select(['athlete_id', 'sex', 'height', 'weight'])\n",
    "athlete_bio_short_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Load `athlete_event_results`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "athlete_event_results_raw_df = get_jdbc_df(\"athlete_event_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----------+---------+-------------+---------+--------------+----------+------+-----+-----------+\n",
      "|           edition|edition_id|country_noc|    sport|        event|result_id|       athlete|athlete_id|   pos|medal|isTeamSport|\n",
      "+------------------+----------+-----------+---------+-------------+---------+--------------+----------+------+-----+-----------+\n",
      "|1908SummerOlympics|         5|        ANZ|Athletics|100metres,Men|    56265|ErnestHutcheon|     64710|   DNS|  nan|      False|\n",
      "|1908SummerOlympics|         5|        ANZ|Athletics|400metres,Men|    56313|   HenryMurray|     64756|   DNS|  nan|      False|\n",
      "|1908SummerOlympics|         5|        ANZ|Athletics|800metres,Men|    56338|  HarveySutton|     64808|3h8r12|  nan|      False|\n",
      "|1908SummerOlympics|         5|        ANZ|Athletics|800metres,Men|    56338|    GuyHaskins|    922519|   DNS|  nan|      False|\n",
      "|1908SummerOlympics|         5|        ANZ|Athletics|800metres,Men|    56338|   JosephLynch|     64735|   DNS|  nan|      False|\n",
      "+------------------+----------+-----------+---------+-------------+---------+--------------+----------+------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "athlete_event_results_raw_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316834"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athlete_event_results_raw_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|d4fd9d75-b0da-4a3...|{\"edition\":\"1908S...|\n",
      "|9d55030c-d7d6-420...|{\"edition\":\"1908S...|\n",
      "|b66f244f-a926-4de...|{\"edition\":\"1908S...|\n",
      "|fb235cae-bfe0-485...|{\"edition\":\"1908S...|\n",
      "|55aa1d47-d304-48d...|{\"edition\":\"1908S...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = athlete_event_results_raw_df.columns\n",
    "athlete_event_results_df = athlete_event_results_raw_df.withColumn(\"key\", expr(\"uuid()\"))\n",
    "athlete_event_results_json_df = athlete_event_results_df.select(\n",
    "        'key',\n",
    "        to_json(struct(columns)).alias('value')\n",
    "    )\n",
    "athlete_event_results_json_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "athlete_event_results_json_df \\\n",
    "    .write \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', kafka_config['bootstrap_servers'][0]) \\\n",
    "        .option('topic', topic_names[TOPIC_EVENT_RESULTS_IX]) \\\n",
    "        .option('kafka.security.protocol', 'SASL_PLAINTEXT') \\\n",
    "        .option('kafka.sasl.mechanism', 'PLAIN') \\\n",
    "        .option('kafka.sasl.jaas.config',\n",
    "            f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_config['username']}\" password=\"{kafka_config['password']}\";') \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![write_to_kafka1](screenshots/write_to_kafka1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = StructType([\n",
    "        StructField('edition', StringType(), True),\n",
    "        StructField('edition_id', IntegerType(), True),\n",
    "        StructField('country_noc', StringType(), True),\n",
    "        StructField('sport', StringType(), True),\n",
    "        StructField('event', StringType(), True),\n",
    "        StructField('result_id', StringType(), True),\n",
    "        StructField('athlete', StringType(), True),\n",
    "        StructField('athlete_id', IntegerType(), True),\n",
    "        StructField('pos', StringType(), True),\n",
    "        StructField('medal', StringType(), True),\n",
    "        StructField('isTeamSport', StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', kafka_config['bootstrap_servers'][0]) \\\n",
    "        .option('kafka.security.protocol', 'SASL_PLAINTEXT') \\\n",
    "        .option('kafka.sasl.mechanism', 'PLAIN') \\\n",
    "        .option('kafka.sasl.jaas.config',\n",
    "                f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_config['username']}\" password=\"{kafka_config['password']}\";') \\\n",
    "        .option('subscribe', topic_names[TOPIC_EVENT_RESULTS_IX]) \\\n",
    "        .option('startingOffsets', 'earliest') \\\n",
    "        .option('maxOffsetsPerTrigger', '300') \\\n",
    "        .load()\n",
    "\n",
    "clean_df = df.selectExpr('CAST(value AS STRING) AS value_deserialized') \\\n",
    "        .withColumn('value_json', from_json(col('value_deserialized'), json_schema)) \\\n",
    "        .withColumn('edition', col('value_json.edition')) \\\n",
    "        .withColumn('edition_id', col('value_json.edition_id')) \\\n",
    "        .withColumn('country_noc', col('value_json.country_noc')) \\\n",
    "        .withColumn('sport', col('value_json.sport')) \\\n",
    "        .withColumn('event', col('value_json.event')) \\\n",
    "        .withColumn('result_id', col('value_json.result_id')) \\\n",
    "        .withColumn('athlete', col('value_json.athlete')) \\\n",
    "        .withColumn('athlete_id', col('value_json.athlete_id')) \\\n",
    "        .withColumn('pos', col('value_json.pos')) \\\n",
    "        .withColumn('medal', col('value_json.medal')) \\\n",
    "        .withColumn('isTeamSport', col('value_json.isTeamSport')) \\\n",
    "        .drop('value_json', 'value_deserialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Join `athlete_event_results` with `athlete_bio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = clean_df.join(athlete_bio_short_df, 'athlete_id', 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Calculate average height and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = joined_df \\\n",
    "        .groupBy('sport', 'medal', 'sex', 'country_noc') \\\n",
    "        .agg(\n",
    "                round(avg('height'), 4).alias('avg_height'),\n",
    "                round(avg('weight'), 4).alias('avg_weight'),\n",
    "        ) \\\n",
    "        .withColumn('timestamp', lit(str(datetime.datetime.now())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Write streaming data to Kafka and MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 00:25:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:49 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:50 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/01/31 00:25:51 WARN HDFSBackedStateStoreProvider: The state for version 92 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def foreach_batch_function(batch_df, batch_id):\n",
    "\n",
    "    # convert to json\n",
    "    columns = batch_df.columns\n",
    "    batch_json_df = batch_df.withColumn(\"key\", expr(\"uuid()\"))\n",
    "    batch_json_df = batch_json_df.select(\n",
    "            'key',\n",
    "            to_json(struct(columns)).alias('value')\n",
    "        )\n",
    "\n",
    "    # batch_json_df.show()\n",
    "\n",
    "    # send to kafka\n",
    "    batch_json_df.write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option('kafka.bootstrap.servers', kafka_config['bootstrap_servers'][0]) \\\n",
    "        .option('topic', topic_names[TOPIC_GROUP_RESULTS_IX]) \\\n",
    "        .option('kafka.security.protocol', 'SASL_PLAINTEXT') \\\n",
    "        .option('kafka.sasl.mechanism', 'PLAIN') \\\n",
    "        .option('kafka.sasl.jaas.config',\n",
    "            f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_config['username']}\" password=\"{kafka_config['password']}\";') \\\n",
    "        .save()\n",
    "\n",
    "    # send to mysql\n",
    "    batch_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f'{jdbc_config['url']}/{kafka_config['name']}') \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"dbtable\", 'athlete_enriched_agg') \\\n",
    "        .option(\"user\", jdbc_config['user']) \\\n",
    "        .option(\"password\", jdbc_config['password']) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "\n",
    "stream = grouped_df.writeStream \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option('checkpointLocation', f'/tmp/checkpoints-{topic_names[TOPIC_GROUP_RESULTS_IX]}-display1') \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 00:25:54 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16518,Thread-16236,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16511,Thread-16229,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16519,Thread-16237,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16521,Thread-16239,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16517,Thread-16235,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16522,Thread-16240,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16504,Thread-16222,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16523,Thread-16241,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16525,Thread-16243,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16526,Thread-16244,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16524,Thread-16242,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16530,Thread-16248,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN Shell: Interrupted while joining on: Thread[#16529,Thread-16247,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait0(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:366)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2079)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:2155)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:73)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:74)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 3.0 in stage 38.0 (TID 2621) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 12.0 in stage 38.0 (TID 2630) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 0.0 in stage 38.0 (TID 2618) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 11.0 in stage 38.0 (TID 2629) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 8.0 in stage 38.0 (TID 2626) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 9.0 in stage 38.0 (TID 2627) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 15.0 in stage 38.0 (TID 2633) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 10.0 in stage 38.0 (TID 2628) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 14.0 in stage 38.0 (TID 2632) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 1.0 in stage 38.0 (TID 2619) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 4.0 in stage 38.0 (TID 2622) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 7.0 in stage 38.0 (TID 2625) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 5.0 in stage 38.0 (TID 2623) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 6.0 in stage 38.0 (TID 2624) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 13.0 in stage 38.0 (TID 2631) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n",
      "25/01/31 00:26:21 WARN TaskSetManager: Lost task 2.0 in stage 38.0 (TID 2620) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job 11 cancelled part of cancelled job group 07fac833-08d8-42f4-adf2-6828adf18393)\n"
     ]
    }
   ],
   "source": [
    "time.sleep(30)\n",
    "stream.stop()\n",
    "stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kafka\n",
    "\n",
    "![write_to_kafka2](./screenshots/write_to_kafka2.png)\n",
    "\n",
    "##### MySQL\n",
    "\n",
    "![write_to_mysql](./screenshots/write_to_mysql.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env3.12_mds_de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
